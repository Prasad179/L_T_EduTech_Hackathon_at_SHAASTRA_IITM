{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **L&T EduTech Hackathon at SHAASTRA IITM**\n",
        "\n",
        "Here we will try to find optimum training data in percentage with the help of pretrained model that we selected earlier. The judging metric is Cohen Kappa score. \n",
        "\n",
        "### Kappa Score:\n",
        "Cohenâ€™s kappa measures the agreement between two raters who each classify N items into C mutually exclusive categories.\n",
        "\n",
        "More on Cohen kappa score can be found [here](https://towardsdatascience.com/cohens-kappa-9786ceceab58)."
      ],
      "metadata": {
        "id": "w8NCbpMeV7Su"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXMTdP4HVtdv",
        "outputId": "50839f17-1a1e-469e-d41d-636f1257231b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from opendatasets) (4.64.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from opendatasets) (7.1.2)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (from opendatasets) (1.5.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (7.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2022.12.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (2.25.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle->opendatasets) (1.24.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle->opendatasets) (4.0.0)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from keras import layers\n",
        "import keras.backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Input, Dense, Activation, Dropout\n",
        "from keras.layers import Flatten, BatchNormalization, Conv2D\n",
        "from keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D \n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from numpy import expand_dims\n",
        "\n",
        "!pip install opendatasets\n",
        "import opendatasets as od"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "od.download('https://www.kaggle.com/datasets/arpitjain007/game-of-deep-learning-ship-datasets/code')"
      ],
      "metadata": {
        "id": "6GEF0RpsbJzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = r'/content/game-of-deep-learning-ship-datasets'  # main data folder path\n",
        "train_path = os.path.join(dataset_path, 'train')\n",
        "images_path = os.path.join(train_path, 'images')\n",
        "test_csv = os.path.join(dataset_path, 'test_ApKoW4T.csv')\n",
        "train_csv = os.path.join(train_path, 'train.csv')\n",
        "\n",
        "test_df = pd.read_csv(test_csv)\n",
        "train_df = pd.read_csv(train_csv) # it contains image names and their respective labels\n",
        "\n",
        "num_test_img = len(test_df)\n",
        "num_train_img = len(train_df)\n",
        "\n",
        "print(f'Total Number of test images: {num_test_img}')\n",
        "print(f'Total Number of train images: {num_train_img}')"
      ],
      "metadata": {
        "id": "bL2BcrsCbLZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = \"/content/game-of-deep-learning-ship-datasets/train/images/\"   # it contains all the images\n",
        "main_df = pd.read_csv(r'/content/game-of-deep-learning-ship-datasets/train/train.csv')\n",
        "paths = os.listdir(main_path)\n",
        "main_df['path'] = main_path + main_df['image']    # In the path column, paths of all images saved.\n",
        "\n",
        "categories = list(main_df['category'])   # Saves list of ship categories.\n",
        "categorys = {1:'Cargo', 2:'Military', 3:'Carrier', 4:'Cruise', 5:'Tankers'}"
      ],
      "metadata": {
        "id": "FLjWKnoWbP4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = []\n",
        "for category in categories:\n",
        "    classes.append(categorys[category])         # Ship category is saved in classes sequencially.\n",
        "\n",
        "\n",
        "main_df['classes'] = classes\n",
        "test_df = pd.read_csv(r'/content/game-of-deep-learning-ship-datasets/test_ApKoW4T.csv')\n",
        "test_df['path'] = main_path + test_df['image']"
      ],
      "metadata": {
        "id": "x9T7Oxayb1XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "widths, heights = [], []\n",
        "\n",
        "for path in tqdm(main_df[\"path\"]):\n",
        "    width, height = Image.open(path).size\n",
        "    widths.append(width)\n",
        "    heights.append(height)\n",
        "    \n",
        "main_df[\"width\"] = widths\n",
        "main_df[\"height\"] = heights\n",
        "main_df[\"dimension\"] = main_df[\"width\"] * main_df[\"height\"]"
      ],
      "metadata": {
        "id": "vrraLouOdizT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = main_df[['path', 'classes']], main_df['classes']\n",
        "\n",
        "X_data, X_test, y_data, y_test = train_test_split(X, y, test_size=0.1, random_state=42)  # 10% test and validation data\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)  # 5% test data and 5% validation data"
      ],
      "metadata": {
        "id": "VVpLzLrldvuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rotation_range=20,\n",
        "    zoom_range=0.10,\n",
        "    brightness_range=[0.6,1.4],\n",
        "    channel_shift_range=0.7,\n",
        "    width_shift_range=0.15,\n",
        "    height_shift_range=0.15,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ") \n",
        "\n",
        "val_generator = datagen.flow_from_dataframe(\n",
        "        X_val,  # This is the source directory for training images\n",
        "        x_col='path',\n",
        "        y_col='classes',\n",
        "        target_size=(224, 224),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=True,\n",
        ")\n",
        "test_generator = datagen.flow_from_dataframe(\n",
        "        X_test,  # This is the source directory for training images\n",
        "        x_col='path',\n",
        "        y_col='classes',\n",
        "        target_size=(224, 224),  # All images will be resized to 150x150\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "id": "Os2lAQ05iLdx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.xception import Xception\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "xception_model = Xception(include_top=False, weights='imagenet', input_shape=IMG_SHAPE)"
      ],
      "metadata": {
        "id": "D29BKCqLiLgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_size1 = [1,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0]\n",
        "from keras.models import load_model\n",
        "\n",
        "kappa_score = {}\n",
        "saved_models = ['model10.h5','model20.h5','model30.h5','model40.h5','model50.h5','model60.h5','model70.h5','model80.h5','model90.h5','model100.h5']\n",
        "X_train = {}\n",
        "X_v = {}\n",
        "y_train= {}\n",
        "y_v = {}\n",
        "for i in range(len(test_size1)):\n",
        "  X_train[i], X_v[i], y_train[i], y_v[i] = train_test_split(X_data, y_data, test_size= test_size1[i]/0.9, random_state=42)\n",
        "  \n",
        "  train_generator = datagen.flow_from_dataframe(\n",
        "        X_train[i],  # This is the source directory for training images\n",
        "        x_col='path',\n",
        "        y_col='classes',\n",
        "        target_size=(224, 224),  # All images will be resized to 224x224\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle=True,\n",
        "        )\n",
        "\n",
        "  model_xception = tf.keras.Sequential([\n",
        "  xception_model,\n",
        "  tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(5, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model_xception.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "  history_xception = model_xception.fit(\n",
        "      train_generator,\n",
        "      validation_data=val_generator,\n",
        "      epochs=20,\n",
        "      verbose=2)\n",
        "\n",
        "  model_xception.save(saved_models[i])\n",
        "\n",
        "  y_pred = model_xception.predict(test_generator)\n",
        "\n",
        "  y_pred_classes = np.argmax(y_pred, axis = 1)\n",
        "  print(\"\\nAccuracy:\",metrics.accuracy_score(test_generator.labels, y_pred_classes))\n",
        "  print('\\nF1 Score is',f1_score(test_generator.labels, y_pred_classes, average='weighted'))\n",
        "  kappa_score[i] = cohen_kappa_score(test_generator.labels, y_pred_classes)\n",
        "  print('\\nCohen Kappa Score is\\n ',kappa_score[i])"
      ],
      "metadata": {
        "id": "V8sT9ax6iHQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_per = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
        "kappa_score = np.array(list(kappa_score.items())).T[1]\n",
        "plt.plot(train_data_per,kappa_score)"
      ],
      "metadata": {
        "id": "SvS4GMBFwfkr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}